{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e63eb2d-445b-4213-a9c7-43521ef0593b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 18:44:09.058099: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-05 18:44:09.058136: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from utils.callbacks import TensorBoard\n",
    "from utils.util import *\n",
    "from utils.args_loader import load_model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456274ef-2a4e-4310-a8a3-ea950aad7590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(\"/mnt/disks/KITTI/small/\")\n",
    "data_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66db2f5b-fbb2-42f4-8de5-5d84049cb50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32055059-cefe-4f49-b3ed-051a457392a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 18:44:11.284441: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-05 18:44:11.285195: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-05 18:44:11.285266: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-09-05 18:44:11.285328: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-09-05 18:44:11.285391: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-09-05 18:44:11.285457: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-09-05 18:44:11.285518: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-09-05 18:44:11.285581: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-09-05 18:44:11.285642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-09-05 18:44:11.285654: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-09-05 18:44:11.286214: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "arg = SimpleNamespace(model=\"squeezesegv2\",\n",
    "                      config=\"squeezesegv2kitti\",\n",
    "                      data_path=data_path,\n",
    "                      train_dir=\"../output\",\n",
    "                      epochs=10)   \n",
    "\n",
    "config, model = load_model_config(arg.model, arg.config)\n",
    "# config[\"DATA_AUGMENTATION\"]=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90245425-2c65-429d-9b85-a3b914a0b5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFRecord exists at /mnt/disks/KITTI/small/train.tfrecord. Skipping TFRecord writing.\n",
      "TFRecord exists at /mnt/disks/KITTI/small/val.tfrecord. Skipping TFRecord writing.\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(\"train\", arg.data_path, config).write_tfrecord_dataset().read_tfrecord_dataset()\n",
    "val_dl = DataLoader(\"val\", arg.data_path, config).write_tfrecord_dataset().read_tfrecord_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e119c85-9b06-4f72-a0f0-95fd791e9a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=config.LEARNING_RATE,\n",
    "    decay_steps=config.LR_DECAY_STEPS,\n",
    "    decay_rate=config.LR_DECAY_FACTOR,\n",
    "    staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b78fc039-6adc-4b1c-8e56-c38c6bd77520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([8, 64, 1024, 6]), TensorShape([8, 64, 1024]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lidar_input, lidar_mask), label, weight  = train_dl.take(1).get_single_element()\n",
    "\n",
    "lidar_input.shape, lidar_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "785d35e9-fe17-4ded-bc8f-36f7537f2b7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You cannot build your model by calling `build` if your layers do not support float type inputs. Instead, in order to instantiate and build your model, call your model on real tensor data (of the correct dtype).\n\nThe actual error from `call` is: Input 'condition' of 'SelectV2' Op has type float32 that does not match expected type of bool..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/tf/lib/python3.10/site-packages/keras/engine/training.py:449\u001b[0m, in \u001b[0;36mModel.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mInvalidArgumentError, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/wandb/PCLSegmentation/pcl_segmentation/nets/SqueezeSegV2.py:325\u001b[0m, in \u001b[0;36mSqueezeSegV2.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    323\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv14(x)\n\u001b[0;32m--> 325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegmentation_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlidar_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wandb/PCLSegmentation/pcl_segmentation/nets/SegmentationNetwork.py:65\u001b[0m, in \u001b[0;36mPCLSegmentationNetwork.segmentation_head\u001b[0;34m(self, logits, lidar_mask)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;66;03m# set predictions to the \"None\" class where no points are present\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m   predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlidar_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASSES\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                         \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probabilities, predictions\n",
      "File \u001b[0;32m~/mambaforge/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py:579\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[0;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_arg\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m types_pb2\u001b[38;5;241m.\u001b[39mDT_INVALID:\n\u001b[0;32m--> 579\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected type of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtypes\u001b[38;5;241m.\u001b[39mas_dtype(input_arg\u001b[38;5;241m.\u001b[39mtype)\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m   \u001b[38;5;66;03m# Update the maps with the default, if needed.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input 'condition' of 'SelectV2' Op has type float32 that does not match expected type of bool.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/tf/lib/python3.10/site-packages/keras/engine/training.py:451\u001b[0m, in \u001b[0;36mModel.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    449\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mInvalidArgumentError, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 451\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou cannot build your model by calling `build` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    452\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mif your layers do not support float type inputs. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    453\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstead, in order to instantiate and build your \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    454\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel, call your model on real tensor data (of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    455\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe correct dtype).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe actual error from \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    456\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`call` is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28msuper\u001b[39m(Model, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mbuild(input_shape)\n",
      "\u001b[0;31mValueError\u001b[0m: You cannot build your model by calling `build` if your layers do not support float type inputs. Instead, in order to instantiate and build your model, call your model on real tensor data (of the correct dtype).\n\nThe actual error from `call` is: Input 'condition' of 'SelectV2' Op has type float32 that does not match expected type of bool.."
     ]
    }
   ],
   "source": [
    "model.build([[8, 64, 1024, 6], [8, 64, 1024]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3739e314-266a-4780-8613-03890204909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model([lidar_input, lidar_mask])\n",
    "\n",
    "model.save(\"model.tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a8e3343-76dc-4bcb-a50d-1ee1dd89c4f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You cannot build your model by calling `build` if your layers do not support float type inputs. Instead, in order to instantiate and build your model, call your model on real tensor data (of the correct dtype).\n\nThe actual error from `call` is: Input 'condition' of 'SelectV2' Op has type float32 that does not match expected type of bool..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/tf/lib/python3.10/site-packages/keras/engine/training.py:449\u001b[0m, in \u001b[0;36mModel.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mInvalidArgumentError, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/wandb/PCLSegmentation/pcl_segmentation/nets/SqueezeSegV2.py:325\u001b[0m, in \u001b[0;36mSqueezeSegV2.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    323\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv14(x)\n\u001b[0;32m--> 325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegmentation_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlidar_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wandb/PCLSegmentation/pcl_segmentation/nets/SegmentationNetwork.py:65\u001b[0m, in \u001b[0;36mPCLSegmentationNetwork.segmentation_head\u001b[0;34m(self, logits, lidar_mask)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;66;03m# set predictions to the \"None\" class where no points are present\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m   predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlidar_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASSES\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                         \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probabilities, predictions\n",
      "File \u001b[0;32m~/mambaforge/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py:579\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[0;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_arg\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m types_pb2\u001b[38;5;241m.\u001b[39mDT_INVALID:\n\u001b[0;32m--> 579\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected type of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtypes\u001b[38;5;241m.\u001b[39mas_dtype(input_arg\u001b[38;5;241m.\u001b[39mtype)\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    582\u001b[0m   \u001b[38;5;66;03m# Update the maps with the default, if needed.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input 'condition' of 'SelectV2' Op has type float32 that does not match expected type of bool.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlidar_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlidar_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/tf/lib/python3.10/site-packages/keras/engine/training.py:451\u001b[0m, in \u001b[0;36mModel.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    449\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall(x, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (tf\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mInvalidArgumentError, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 451\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou cannot build your model by calling `build` \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    452\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mif your layers do not support float type inputs. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    453\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstead, in order to instantiate and build your \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    454\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel, call your model on real tensor data (of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    455\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe correct dtype).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe actual error from \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    456\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`call` is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28msuper\u001b[39m(Model, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mbuild(input_shape)\n",
      "\u001b[0;31mValueError\u001b[0m: You cannot build your model by calling `build` if your layers do not support float type inputs. Instead, in order to instantiate and build your model, call your model on real tensor data (of the correct dtype).\n\nThe actual error from `call` is: Input 'condition' of 'SelectV2' Op has type float32 that does not match expected type of bool.."
     ]
    }
   ],
   "source": [
    "model.build([lidar_input.shape, lidar_mask.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5534d-5698-4f44-99f4-8ea57b8c7f41",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Let's log some data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4934aed0-47fa-499c-93f6-9438ab34d9d6",
   "metadata": {},
   "source": [
    "We can use W&B to view the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02f093c9-a1b6-482d-a547-8f2bd9cb5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(img, factor=3):\n",
    "    return tf.image.resize(img, [img.shape[0]*factor, img.shape[1]*factor]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faaa0a71-64fa-4429-83f7-dbaf36cc6dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_input_data(lidar_input, label, class_color_map):\n",
    "    \"Log inputs to wandb\"\n",
    "    label = label[:, :]\n",
    "    label_image = class_color_map[label.numpy().reshape(-1)].reshape([label.shape[0], label.shape[1], 3])\n",
    "    depth_image = lidar_input.numpy()[:, :, [4]]\n",
    "    intensity = lidar_input.numpy()[:, :, [3]]\n",
    "    points = lidar_input.numpy()[:, :, :3]\n",
    "    points_rgb = np.concatenate([points, (255*label_image).astype(int)], axis=-1).reshape(-1, 6)\n",
    "    \n",
    "    depth_image, label_image, intensity_image  = map(resize, [depth_image, label_image, intensity])\n",
    "    \n",
    "    # log 2 wandb\n",
    "    wandb.log({'Images/Label Image': wandb.Image(label_image)})\n",
    "    wandb.log({'Images/Depth Image': wandb.Image(depth_image)})\n",
    "    wandb.log({'Images/Intensity Image': wandb.Image(intensity_image)})\n",
    "    wandb.log({\"Images/3D\": wandb.Object3D({\"type\": \"lidar/beta\", \"points\":points_rgb})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4363b3f0-6249-4c3d-afc0-98207bd012f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tcapelle/wandb/PCLSegmentation/pcl_segmentation/wandb/run-20220905_145529-17sal5u5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/av-team/small_kitti/runs/17sal5u5\" target=\"_blank\">driven-snow-2</a></strong> to <a href=\"https://wandb.ai/av-team/small_kitti\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logging image: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.066 MB of 0.374 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.176348â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">driven-snow-2</strong>: <a href=\"https://wandb.ai/av-team/small_kitti/runs/17sal5u5\" target=\"_blank\">https://wandb.ai/av-team/small_kitti/runs/17sal5u5</a><br/>Synced 6 W&B file(s), 4 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220905_145529-17sal5u5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=\"small_kitti\", entity=\"av-team\", job_type=\"log_dataset\"):\n",
    "    (lidar_inputs, lidar_masks), labels, weights = val_dl.take(1).get_single_element() # a batch of 32 images\n",
    "    for i, (lidar_input, label) in enumerate(zip(lidar_inputs, labels)):\n",
    "        print(f\"logging image: {i}\")\n",
    "        log_input_data(lidar_input, label, model.CLS_COLOR_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29b3a5ed-92db-464f-bbd1-4bb5a8911a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "\n",
    "\n",
    "class LogSamplesCallback(WandbCallback):\n",
    "    \"A simple Keras callback to log model predictions\"\n",
    "    \n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dataset = dataset\n",
    "        self.num_images = 1\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super().on_epoch_end(epoch, logs=logs)\n",
    "        \n",
    "        # get first batch of dataset\n",
    "        class_color_map = self.model.CLS_COLOR_MAP\n",
    "        (lidar_input, lidar_mask), label, weight = self.dataset.take(1).get_single_element()\n",
    "\n",
    "        probabilities, predictions = self.model([lidar_input, lidar_mask])\n",
    "        predictions = predictions[:self.num_images, :, :].numpy()\n",
    "\n",
    "        label = label[:self.num_images, :, :]\n",
    "        \n",
    "\n",
    "        # label and prediction visualizations\n",
    "        label_image = class_color_map[label.numpy().reshape(-1)].reshape([self.num_images, label.shape[1], label.shape[2], 3])\n",
    "        pred_image = class_color_map[predictions.reshape(-1)].reshape([self.num_images, label.shape[1], label.shape[2], 3])\n",
    "        depth_image = lidar_input.numpy()[:self.num_images, :, :, [4]]\n",
    "        intensity = lidar_input.numpy()[:self.num_images, :, :, [3]]\n",
    "        points = lidar_input.numpy()[:self.num_images, :, :, :3]\n",
    "        points_rgb = np.concatenate([points, (255*label_image).astype(int)], axis=-1).reshape(-1, 6)\n",
    "        \n",
    "        def _resize(img, factor=3):\n",
    "            return tf.image.resize(img, [img.shape[1]*factor, img.shape[2]*factor]).numpy()\n",
    "        intensity_image, depth_image, weight_image,label_image, pred_image = map(_resize, [intensity, depth_image, weight_image,label_image, pred_image])\n",
    "        \n",
    "        # log a bunch of images\n",
    "        wandb.log({'Images/Label Image': wandb.Image(label_image)}, step=epoch)\n",
    "        wandb.log({'Images/Depth Image': wandb.Image(depth_image)}, step=epoch)\n",
    "        wandb.log({'Images/Intensity Image': wandb.Image(intensity_image)}, step=epoch)\n",
    "        wandb.log({'Images/Prediction Image':wandb.Image(pred_image)}, step=epoch)\n",
    "        wandb.log({\"Images/3D\": wandb.Object3D({\"type\": \"lidar/beta\", \"points\":points_rgb})})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6259a4a4-fb4d-4998-aa58-7bec32f5ff3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tcapelle/wandb/PCLSegmentation/pcl_segmentation/wandb/run-20220905_145642-1076dh6x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/av-team/small_kitti/runs/1076dh6x\" target=\"_blank\">lunar-spaceship-3</a></strong> to <a href=\"https://wandb.ai/av-team/small_kitti\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"small_kitti\", entity=\"av-team\", sync_tensorboard=True)\n",
    "\n",
    "wandb_callback = WandbCallback(dataset=val_dl, save_model=False)\n",
    "# tensorboard_callback = TensorBoard(arg.train_dir, val_dl, profile_batch=(200, 202))\n",
    "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(os.path.join(arg.train_dir, \"checkpoint\"))\n",
    "\n",
    "# cbs = [wandb_callback, tensorboard_callback, checkpoint_callback]\n",
    "cbs = [wandb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98463e0e-be3c-4086-8629-fc9de8430118",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=config.MAX_GRAD_NORM)\n",
    "\n",
    "model.compile(optimizer=optimizer, weighted_metrics=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6449269-22df-421f-a83d-59bcc007e3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to compute FLOPs for this model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 14:57:00.897335: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1073741824 exceeds 10% of free system memory.\n",
      "2022-09-05 14:57:01.457100: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 268435456 exceeds 10% of free system memory.\n",
      "2022-09-05 14:57:01.639472: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1073741824 exceeds 10% of free system memory.\n",
      "2022-09-05 14:57:02.602553: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 100663296 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 59s 59s/step - loss: 3.0744 - miou: 0.0546"
     ]
    }
   ],
   "source": [
    "model.fit(train_dl,\n",
    "        validation_data=val_dl,\n",
    "        epochs=arg.epochs,\n",
    "        callbacks = cbs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880e934-9bb9-49d3-a1c2-5dd2030b230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
